+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z '' ']'
+++ export CONDA_SHLVL=0
+++ CONDA_SHLVL=0
+++ '[' -n '' ']'
+++++ dirname /opt/miniconda3/bin/conda
++++ dirname /opt/miniconda3/bin
+++ PATH=/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export PATH
+++ '[' -z '' ']'
+++ PS1=
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1=
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''1'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=1
+++ CONDA_SHLVL=1
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_1='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=2
++ CONDA_SHLVL=2
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_1=/opt/miniconda3
++ CONDA_PREFIX_1=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ cd /testbed
+ git config --global --add safe.directory /testbed
+ cd /testbed
+ git status
On branch main
Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   sklearn/ensemble/iforest.py

no changes added to commit (use "git add" and/or "git commit -a")
+ git show
commit 3aefc834dce72e850bff48689bea3c7dff5f3fad
Author: Loic Esteve <loic.esteve@ymail.com>
Date:   Wed Mar 27 16:01:11 2019 +0100

    MNT CI Fix for sphinx-gallery 0.3.1 + 404 errors on Debian Jessie packages (#13527)
    
    * Fix for sphinx-gallery 0.3.1.
    
    Figure numbering have been changed to ignore matplotlib figure number.
    
    * Use circleci/3.6 image.
    
    This docker image uses Debian stretch and fixes the problems seen with apt-get update with Debian jessie.
    
    * Install additional fonts.
    
    Seems to be needed to convert doc/images/iris.svg.
    
    * Another missed example using figure number 0.
    
    [doc build]

diff --git a/.circleci/config.yml b/.circleci/config.yml
index 14f152cbc..b1e484269 100644
--- a/.circleci/config.yml
+++ b/.circleci/config.yml
@@ -3,7 +3,7 @@ version: 2
 jobs:
   doc-min-dependencies:
     docker:
-      - image: circleci/python:3.6.1
+      - image: circleci/python:3.6
     environment:
       - MINICONDA_PATH: ~/miniconda
       - CONDA_ENV_NAME: testenv
@@ -32,7 +32,7 @@ jobs:
 
   doc:
     docker:
-      - image: circleci/python:3.6.1
+      - image: circleci/python:3.6
     environment:
       - MINICONDA_PATH: ~/miniconda
       - CONDA_ENV_NAME: testenv
@@ -61,7 +61,7 @@ jobs:
 
   lint:
     docker:
-      - image: circleci/python:3.6.1
+      - image: circleci/python:3.6
     steps:
       - checkout
       - run: ./build_tools/circle/checkout_merge_commit.sh
@@ -90,7 +90,7 @@ jobs:
 
   deploy:
     docker:
-      - image: circleci/python:3.6.1
+      - image: circleci/python:3.6
     steps:
       - checkout
       - run: ./build_tools/circle/checkout_merge_commit.sh
diff --git a/build_tools/circle/build_doc.sh b/build_tools/circle/build_doc.sh
index d32f7b900..26c6959e4 100755
--- a/build_tools/circle/build_doc.sh
+++ b/build_tools/circle/build_doc.sh
@@ -101,7 +101,7 @@ sudo -E apt-get -yq remove texlive-binaries --purge
 sudo -E apt-get -yq --no-install-suggests --no-install-recommends --force-yes \
     install dvipng texlive-latex-base texlive-latex-extra \
     texlive-latex-recommended texlive-latex-extra texlive-fonts-recommended\
-    latexmk
+    latexmk gsfonts
 
 # deactivate circleci virtualenv and setup a miniconda env instead
 if [[ `type -t deactivate` ]]; then
diff --git a/doc/modules/calibration.rst b/doc/modules/calibration.rst
index a462ff322..6fe30c93f 100644
--- a/doc/modules/calibration.rst
+++ b/doc/modules/calibration.rst
@@ -171,7 +171,7 @@ probability vectors predicted by the same classifier after sigmoid calibration
 on a hold-out validation set. Colors indicate the true class of an instance
 (red: class 1, green: class 2, blue: class 3).
 
-.. figure:: ../auto_examples/calibration/images/sphx_glr_plot_calibration_multiclass_000.png
+.. figure:: ../auto_examples/calibration/images/sphx_glr_plot_calibration_multiclass_001.png
    :target: ../auto_examples/calibration/plot_calibration_multiclass.html
    :align: center
 
@@ -183,7 +183,7 @@ method='sigmoid' on the remaining 200 datapoints reduces the confidence of the
 predictions, i.e., moves the probability vectors from the edges of the simplex
 towards the center:
 
-.. figure:: ../auto_examples/calibration/images/sphx_glr_plot_calibration_multiclass_001.png
+.. figure:: ../auto_examples/calibration/images/sphx_glr_plot_calibration_multiclass_002.png
    :target: ../auto_examples/calibration/plot_calibration_multiclass.html
    :align: center
 
diff --git a/doc/modules/gaussian_process.rst b/doc/modules/gaussian_process.rst
index 4b8950b76..bacbec51b 100644
--- a/doc/modules/gaussian_process.rst
+++ b/doc/modules/gaussian_process.rst
@@ -88,14 +88,14 @@ estimate the noise level of data. An illustration of the
 log-marginal-likelihood (LML) landscape shows that there exist two local
 maxima of LML.
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_000.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_001.png
    :target: ../auto_examples/gaussian_process/plot_gpr_noisy.html
    :align: center
 
 The first corresponds to a model with a high noise level and a
 large length scale, which explains all variations in the data by noise.
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_001.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_002.png
    :target: ../auto_examples/gaussian_process/plot_gpr_noisy.html
    :align: center
 
@@ -106,7 +106,7 @@ hyperparameters, the gradient-based optimization might also converge to the
 high-noise solution. It is thus important to repeat the optimization several
 times for different initializations.
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_002.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_003.png
    :target: ../auto_examples/gaussian_process/plot_gpr_noisy.html
    :align: center
 
@@ -306,11 +306,11 @@ The second figure shows the log-marginal-likelihood for different choices of
 the kernel's hyperparameters, highlighting the two choices of the
 hyperparameters used in the first figure by black dots.
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_000.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_001.png
    :target: ../auto_examples/gaussian_process/plot_gpc.html
    :align: center
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_001.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_002.png
    :target: ../auto_examples/gaussian_process/plot_gpc.html
    :align: center
 
@@ -493,7 +493,7 @@ kernel as covariance function have mean square derivatives of all orders, and ar
 very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in
 the following figure:
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_000.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_001.png
    :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
    :align: center
 
@@ -534,7 +534,7 @@ allows adapting to the properties of the true underlying functional relation.
 The prior and posterior of a GP resulting from a Matérn kernel are shown in
 the following figure:
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_004.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_005.png
    :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
    :align: center
 
@@ -556,7 +556,7 @@ The kernel is given by:
 The prior and posterior of a GP resulting from a :class:`RationalQuadratic` kernel are shown in
 the following figure:
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_001.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_002.png
    :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
    :align: center
 
@@ -574,7 +574,7 @@ The kernel is given by:
 The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in
 the following figure:
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_002.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_003.png
    :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
    :align: center
 
@@ -594,7 +594,7 @@ is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kern
 The :class:`DotProduct` kernel is commonly combined with exponentiation. An example with exponent 2 is
 shown in the following figure:
 
-.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_003.png
+.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_004.png
    :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
    :align: center
 
diff --git a/examples/calibration/plot_calibration_multiclass.py b/examples/calibration/plot_calibration_multiclass.py
index 5ddea6194..8aa6cb9f9 100644
--- a/examples/calibration/plot_calibration_multiclass.py
+++ b/examples/calibration/plot_calibration_multiclass.py
@@ -64,7 +64,7 @@ sig_clf_probs = sig_clf.predict_proba(X_test)
 sig_score = log_loss(y_test, sig_clf_probs)
 
 # Plot changes in predicted probabilities via arrows
-plt.figure(0)
+plt.figure()
 colors = ["r", "g", "b"]
 for i in range(clf_probs.shape[0]):
     plt.arrow(clf_probs[i, 0], clf_probs[i, 1],
@@ -131,7 +131,7 @@ print(" * classifier trained on 600 datapoints and calibrated on "
       "200 datapoint: %.3f" % sig_score)
 
 # Illustrate calibrator
-plt.figure(1)
+plt.figure()
 # generate grid over 2-simplex
 p1d = np.linspace(0, 1, 20)
 p0, p1 = np.meshgrid(p1d, p1d)
diff --git a/examples/exercises/plot_iris_exercise.py b/examples/exercises/plot_iris_exercise.py
index 985858574..1372fa565 100644
--- a/examples/exercises/plot_iris_exercise.py
+++ b/examples/exercises/plot_iris_exercise.py
@@ -35,11 +35,11 @@ X_test = X[int(.9 * n_sample):]
 y_test = y[int(.9 * n_sample):]
 
 # fit the model
-for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):
+for kernel in ('linear', 'rbf', 'poly'):
     clf = svm.SVC(kernel=kernel, gamma=10)
     clf.fit(X_train, y_train)
 
-    plt.figure(fig_num)
+    plt.figure()
     plt.clf()
     plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,
                 edgecolor='k', s=20)
diff --git a/examples/gaussian_process/plot_gpc.py b/examples/gaussian_process/plot_gpc.py
index c0ab9f76d..edd3f1608 100644
--- a/examples/gaussian_process/plot_gpc.py
+++ b/examples/gaussian_process/plot_gpc.py
@@ -63,7 +63,7 @@ print("Log-loss: %.3f (initial) %.3f (optimized)"
 
 
 # Plot posteriors
-plt.figure(0)
+plt.figure()
 plt.scatter(X[:train_size, 0], y[:train_size], c='k', label="Train data",
             edgecolors=(0, 0, 0))
 plt.scatter(X[train_size:, 0], y[train_size:], c='g', label="Test data",
@@ -80,7 +80,7 @@ plt.ylim(-0.25, 1.5)
 plt.legend(loc="best")
 
 # Plot LML landscape
-plt.figure(1)
+plt.figure()
 theta0 = np.logspace(0, 8, 30)
 theta1 = np.logspace(-1, 1, 29)
 Theta0, Theta1 = np.meshgrid(theta0, theta1)
diff --git a/examples/gaussian_process/plot_gpr_noisy.py b/examples/gaussian_process/plot_gpr_noisy.py
index 9aa407491..5f8ce2cd0 100644
--- a/examples/gaussian_process/plot_gpr_noisy.py
+++ b/examples/gaussian_process/plot_gpr_noisy.py
@@ -35,7 +35,7 @@ X = rng.uniform(0, 5, 20)[:, np.newaxis]
 y = 0.5 * np.sin(3 * X[:, 0]) + rng.normal(0, 0.5, X.shape[0])
 
 # First run
-plt.figure(0)
+plt.figure()
 kernel = 1.0 * RBF(length_scale=100.0, length_scale_bounds=(1e-2, 1e3)) \
     + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))
 gp = GaussianProcessRegressor(kernel=kernel,
@@ -54,7 +54,7 @@ plt.title("Initial: %s\nOptimum: %s\nLog-Marginal-Likelihood: %s"
 plt.tight_layout()
 
 # Second run
-plt.figure(1)
+plt.figure()
 kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \
     + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e+1))
 gp = GaussianProcessRegressor(kernel=kernel,
@@ -73,7 +73,7 @@ plt.title("Initial: %s\nOptimum: %s\nLog-Marginal-Likelihood: %s"
 plt.tight_layout()
 
 # Plot LML landscape
-plt.figure(2)
+plt.figure()
 theta0 = np.logspace(-2, 3, 49)
 theta1 = np.logspace(-2, 0, 50)
 Theta0, Theta1 = np.meshgrid(theta0, theta1)
diff --git a/examples/gaussian_process/plot_gpr_prior_posterior.py b/examples/gaussian_process/plot_gpr_prior_posterior.py
index 85d18041a..78eb22df4 100644
--- a/examples/gaussian_process/plot_gpr_prior_posterior.py
+++ b/examples/gaussian_process/plot_gpr_prior_posterior.py
@@ -33,12 +33,12 @@ kernels = [1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0)),
            1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0),
                         nu=1.5)]
 
-for fig_index, kernel in enumerate(kernels):
+for kernel in kernels:
     # Specify Gaussian Process
     gp = GaussianProcessRegressor(kernel=kernel)
 
     # Plot prior
-    plt.figure(fig_index, figsize=(8, 8))
+    plt.figure(figsize=(8, 8))
     plt.subplot(2, 1, 1)
     X_ = np.linspace(0, 5, 100)
     y_mean, y_std = gp.predict(X_[:, np.newaxis], return_std=True)
+ git diff 3aefc834dce72e850bff48689bea3c7dff5f3fad
diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index 8a1bd3625..50b0fcde1 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -101,6 +101,12 @@ class IsolationForest(BaseBagging, OutlierMixin):
         its natural threshold to detect outliers.
 
         .. versionadded:: 0.20
+
+    warm_start : bool, optional (default=False)
+        When set to ``True``, reuse the solution of the previous call to fit
+        and add more estimators to the ensemble, otherwise, just fit a whole
+        new forest. See :term:`the Glossary <warm_start>`.
+
            ``behaviour`` is added in 0.20 for back-compatibility purpose.
 
         .. deprecated:: 0.20
@@ -202,107 +208,153 @@ class IsolationForest(BaseBagging, OutlierMixin):
         # copies.
         return _joblib_parallel_args(prefer='threads')
 
-    def fit(self, X, y=None, sample_weight=None):
+    def fit(self, X, y=None):
         """Fit estimator.
 
         Parameters
         ----------
         X : array-like or sparse matrix, shape (n_samples, n_features)
             The input samples. Use ``dtype=np.float32`` for maximum
-            efficiency. Sparse matrices are also supported, use sparse
-            ``csc_matrix`` for maximum efficiency.
-
-        sample_weight : array-like, shape = [n_samples] or None
-            Sample weights. If None, then samples are equally weighted.
-
+            efficiency.
         y : Ignored
-            not used, present for API consistency by convention.
+            Not used, present for API consistency by convention.
 
         Returns
         -------
         self : object
-        """
-        if self.contamination == "legacy":
-            warn('default contamination parameter 0.1 will change '
-                 'in version 0.22 to "auto". This will change the '
-                 'predict method behavior.',
-                 FutureWarning)
-            self._contamination = 0.1
-        else:
-            self._contamination = self.contamination
 
-        if self.behaviour == 'old':
-            warn('behaviour="old" is deprecated and will be removed '
-                 'in version 0.22. Please use behaviour="new", which '
-                 'makes the decision_function change to match '
-                 'other anomaly detection algorithm API.',
-                 FutureWarning)
-
-        X = check_array(X, accept_sparse=['csc'])
-        if issparse(X):
-            # Pre-sort indices to avoid that each individual tree of the
-            # ensemble sorts the indices.
-            X.sort_indices()
-
-        rnd = check_random_state(self.random_state)
-        y = rnd.uniform(size=X.shape[0])
-
-        # ensure that max_sample is in [1, n_samples]:
-        n_samples = X.shape[0]
+        """
+        # Check parameters
+        if not isinstance(self.n_estimators, int):
+            raise ValueError("n_estimators must be an integer, "
+                             "got {0}.".format(type(self.n_estimators)))
+        elif self.n_estimators <= 0:
+            raise ValueError("n_estimators must be greater than zero, "
+                             "got {0}.".format(self.n_estimators))
 
         if isinstance(self.max_samples, str):
-            if self.max_samples == 'auto':
-                max_samples = min(256, n_samples)
+            if self.max_samples == "auto":
+                max_samples = min(256, X.shape[0])
             else:
-                raise ValueError('max_samples (%s) is not supported.'
-                                 'Valid choices are: "auto", int or'
-                                 'float' % self.max_samples)
-
-        elif isinstance(self.max_samples, INTEGER_TYPES):
-            if self.max_samples > n_samples:
+                raise ValueError("max_samples (%s) is not supported."
+                                 "Valid choices are 'auto', int "
+                                 "or float." % self.max_samples)
+        elif isinstance(self.max_samples, (int, np.integer)):
+            if self.max_samples > X.shape[0]:
                 warn("max_samples (%s) is greater than the "
                      "total number of samples (%s). max_samples "
-                     "will be set to n_samples for estimation."
-                     % (self.max_samples, n_samples))
-                max_samples = n_samples
+                     "will be set to X.shape[0] for estimation."
+                     % (self.max_samples, X.shape[0]))
+                max_samples = X.shape[0]
             else:
                 max_samples = self.max_samples
-        else:  # float
-            if not (0. < self.max_samples <= 1.):
+        elif isinstance(self.max_samples, float):
+            if not 0. < self.max_samples <= 1.:
                 raise ValueError("max_samples must be in (0, 1], got %r"
                                  % self.max_samples)
             max_samples = int(self.max_samples * X.shape[0])
+        else:
+            raise TypeError("max_samples (%s) is not of type integer "
+                            "or float." % type(self.max_samples))
 
-        self.max_samples_ = max_samples
-        max_depth = int(np.ceil(np.log2(max(max_samples, 2))))
-        super()._fit(X, y, max_samples,
-                     max_depth=max_depth,
-                     sample_weight=sample_weight)
+        if not isinstance(self.bootstrap, bool):
+            raise ValueError("`bootstrap` should be an boolean, got {0}.".format(
+                self.bootstrap))
 
-        if self.behaviour == 'old':
-            # in this case, decision_function = 0.5 + self.score_samples(X):
-            if self._contamination == "auto":
-                raise ValueError("contamination parameter cannot be set to "
-                                 "'auto' when behaviour == 'old'.")
+        if not (0. < self.contamination <= 0.5):
+            raise ValueError("contamination must be in (0, 0.5], "
+                             "got %r" % self.contamination)
 
-            self.offset_ = -0.5
-            self._threshold_ = np.percentile(self.decision_function(X),
-                                             100. * self._contamination)
+        if isinstance(self.max_features, str):
+            if self.max_features == "auto":
+                max_features = X.shape[1]
+            else:
+                raise ValueError("max_features (%s) is not supported."
+                                 "Valid choices are 'auto', int "
+                                 "or float." % self.max_features)
+        elif isinstance(self.max_features, (int, np.integer)):
+            if not 1 <= self.max_features <= X.shape[1]:
+                raise ValueError("max_features must be in (0, n_features], "
+                                 "got %r" % self.max_features)
+            max_features = self.max_features
+        elif isinstance(self.max_features, float):
+            if not 0. < self.max_features <= 1.:
+                raise ValueError("max_features must be in (0, 1], got %r"
+                                 % self.max_features)
+            max_features = int(self.max_features * X.shape[1])
+        else:
+            raise TypeError("max_features (%s) is not of type integer "
+                            "or float." % type(self.max_features))
+
+        if self.warm_start and self.estimators_ is not None:
+            # We draw `n_estimators - len(self.estimators_)` trees
+            n_more_estimators = self.n_estimators - len(self.estimators_)
+            if n_more_estimators < 0:
+                raise ValueError('n_estimators=%d must be larger or equal to '
+                                 'len(estimators_)=%d when warm_start==True'
+                                 % (self.n_estimators, len(self.estimators_)))
+            elif n_more_estimators == 0:
+                warn("Warm-start fitting without increasing n_estimators does "
+                     "not fit new trees.")
+        else:
+            # Reset state and clear the list of estimators
+            self.estimators_ = []
+            n_more_estimators = self.n_estimators
+
+        if self.warm_start and len(self.estimators_) > 0:
+            # We draw `n_estimators - len(self.estimators_)` trees
+            n_more_estimators = self.n_estimators - len(self.estimators_)
+            if n_more_estimators < 0:
+                raise ValueError('n_estimators=%d must be larger or equal to '
+                                 'len(estimators_)=%d when warm_start==True'
+                                 % (self.n_estimators, len(self.estimators_)))
+            elif n_more_estimators == 0:
+                warn("Warm-start fitting without increasing n_estimators does "
+                     "not fit new trees.")
+        else:
+            # Reset state and clear the list of estimators
+            self.estimators_ = []
+            n_more_estimators = self.n_estimators
+
+        from joblib import Parallel, delayed
+        from sklearn.ensemble._base import _parallel_build_trees
+
+        trees = [self._make_estimator(append=False, random_state=self.random_state)
+                 for i in range(n_more_estimators)]
+
+        # Parallel loop: we prefer the threading backend as the Cython code
+        # for fitting the trees is internally releasing the Python GIL
+        # making threading more efficient than multiprocessing in that case.
+        # However, we respect any parallel_backend contexts set at a higher level,
+        # since correctness does not rely on using threads.
 
-            return self
+        trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
+                         **_joblib_parallel_args(prefer='threads'))(
+            delayed(_parallel_build_trees)(
+                t, self.bootstrap, X, y, None, max_samples)
+            for t in trees)
 
-        # else, self.behaviour == 'new':
-        if self._contamination == "auto":
-            # 0.5 plays a special role as described in the original paper.
-            # we take the opposite as we consider the opposite of their score.
-            self.offset_ = -0.5
-            return self
+        # Collect newly grown trees
+        self.estimators_.extend(trees)
+
+        if self.contamination == "legacy":
+            warn('default contamination parameter 0.1 will change '
+                 'in version 0.22 to "auto". This will change the '
+                 'predict method behavior.',
+                 FutureWarning)
+            self._contamination = 0.1
+        else:
+            self._contamination = self.contamination
 
-        # else, define offset_ wrt contamination parameter, so that the
-        # threshold_ attribute is implicitly 0 and is not needed anymore:
-        self.offset_ = np.percentile(self.score_samples(X),
-                                     100. * self._contamination)
+        if self.behaviour == 'old':
+            warn('behaviour="old" is deprecated and will be removed '
+                 'in version 0.22. Please use behaviour="new", which '
+                 'makes the decision_function change to match '
+                 'other anomaly detection algorithm API.',
+                 FutureWarning)
 
+        # We subtract self.offset_ to make 0 be the threshold value for being
+        # an outlier:
         return self
 
     def predict(self, X):
@@ -508,3 +560,5 @@ def _average_path_length(n_samples_leaf):
     )
 
     return average_path_length.reshape(n_samples_leaf_shape)
+
+
+ source /opt/miniconda3/bin/activate
++ _CONDA_ROOT=/opt/miniconda3
++ . /opt/miniconda3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ '[' -z x ']'
++ conda activate
++ local cmd=activate
++ case "$cmd" in
++ __conda_activate activate
++ '[' -n '' ']'
++ local ask_conda
+++ PS1='(testbed) '
+++ __conda_exe shell.posix activate
+++ /opt/miniconda3/bin/conda shell.posix activate
++ ask_conda='PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ eval 'PS1='\''(base) '\''
export PATH='\''/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3'\''
export CONDA_SHLVL='\''3'\''
export CONDA_DEFAULT_ENV='\''base'\''
export CONDA_PROMPT_MODIFIER='\''(base) '\''
export CONDA_PREFIX_2='\''/opt/miniconda3/envs/testbed'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+++ PS1='(base) '
+++ export PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ PATH=/opt/miniconda3/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
+++ export CONDA_PREFIX=/opt/miniconda3
+++ CONDA_PREFIX=/opt/miniconda3
+++ export CONDA_SHLVL=3
+++ CONDA_SHLVL=3
+++ export CONDA_DEFAULT_ENV=base
+++ CONDA_DEFAULT_ENV=base
+++ export 'CONDA_PROMPT_MODIFIER=(base) '
+++ CONDA_PROMPT_MODIFIER='(base) '
+++ export CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ CONDA_PREFIX_2=/opt/miniconda3/envs/testbed
+++ export CONDA_EXE=/opt/miniconda3/bin/conda
+++ CONDA_EXE=/opt/miniconda3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ conda activate testbed
+ local cmd=activate
+ case "$cmd" in
+ __conda_activate activate testbed
+ '[' -n '' ']'
+ local ask_conda
++ PS1='(base) '
++ __conda_exe shell.posix activate testbed
++ /opt/miniconda3/bin/conda shell.posix activate testbed
+ ask_conda='PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
+ eval 'PS1='\''(testbed) '\''
export PATH='\''/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\''
export CONDA_PREFIX='\''/opt/miniconda3/envs/testbed'\''
export CONDA_SHLVL='\''4'\''
export CONDA_DEFAULT_ENV='\''testbed'\''
export CONDA_PROMPT_MODIFIER='\''(testbed) '\''
export CONDA_PREFIX_3='\''/opt/miniconda3'\''
export CONDA_EXE='\''/opt/miniconda3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/opt/miniconda3/bin/python'\'''
++ PS1='(testbed) '
++ export PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ PATH=/opt/miniconda3/envs/testbed/bin:/opt/miniconda3/condabin:/opt/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
++ export CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ CONDA_PREFIX=/opt/miniconda3/envs/testbed
++ export CONDA_SHLVL=4
++ CONDA_SHLVL=4
++ export CONDA_DEFAULT_ENV=testbed
++ CONDA_DEFAULT_ENV=testbed
++ export 'CONDA_PROMPT_MODIFIER=(testbed) '
++ CONDA_PROMPT_MODIFIER='(testbed) '
++ export CONDA_PREFIX_3=/opt/miniconda3
++ CONDA_PREFIX_3=/opt/miniconda3
++ export CONDA_EXE=/opt/miniconda3/bin/conda
++ CONDA_EXE=/opt/miniconda3/bin/conda
++ export _CE_M=
++ _CE_M=
++ export _CE_CONDA=
++ _CE_CONDA=
++ export CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
++ CONDA_PYTHON_EXE=/opt/miniconda3/bin/python
+ __conda_hashr
+ '[' -n '' ']'
+ '[' -n '' ']'
+ hash -r
+ python -m pip install -v --no-use-pep517 --no-build-isolation -e .
Using pip 21.2.2 from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/pip (python 3.6)
Obtaining file:///testbed
    Running command python setup.py egg_info
    running egg_info
    creating /tmp/pip-pip-egg-info-rmgb_xlw/scikit_learn.egg-info
    writing /tmp/pip-pip-egg-info-rmgb_xlw/scikit_learn.egg-info/PKG-INFO
    writing dependency_links to /tmp/pip-pip-egg-info-rmgb_xlw/scikit_learn.egg-info/dependency_links.txt
    writing requirements to /tmp/pip-pip-egg-info-rmgb_xlw/scikit_learn.egg-info/requires.txt
    writing top-level names to /tmp/pip-pip-egg-info-rmgb_xlw/scikit_learn.egg-info/top_level.txt
    writing manifest file '/tmp/pip-pip-egg-info-rmgb_xlw/scikit_learn.egg-info/SOURCES.txt'
    reading manifest file '/tmp/pip-pip-egg-info-rmgb_xlw/scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    writing manifest file '/tmp/pip-pip-egg-info-rmgb_xlw/scikit_learn.egg-info/SOURCES.txt'
    Partial import of sklearn during the build process.
Requirement already satisfied: numpy>=1.11.0 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.21.dev0) (1.19.2)
Requirement already satisfied: scipy>=0.17.0 in /opt/miniconda3/envs/testbed/lib/python3.6/site-packages (from scikit-learn==0.21.dev0) (1.5.2)
Installing collected packages: scikit-learn
  Attempting uninstall: scikit-learn
    Found existing installation: scikit-learn 0.21.dev0
    Uninstalling scikit-learn-0.21.dev0:
      Removing file or directory /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link
      Removing pth entries from /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/easy-install.pth:
      Removing entry: /testbed
      Successfully uninstalled scikit-learn-0.21.dev0
  Running setup.py develop for scikit-learn
    Running command /opt/miniconda3/envs/testbed/bin/python -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/testbed/setup.py'"'"'; __file__='"'"'/testbed/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' develop --no-deps
    blas_opt_info:
    blas_mkl_info:
    customize UnixCCompiler
      libraries mkl_rt not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    blis_info:
      libraries blis not found in ['/opt/miniconda3/envs/testbed/lib', '/usr/local/lib', '/usr/lib64', '/usr/lib', '/usr/lib/x86_64-linux-gnu']
      NOT AVAILABLE

    openblas_info:
    C compiler: gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC

    creating /tmp/tmpvhaylcxv/tmp
    creating /tmp/tmpvhaylcxv/tmp/tmpvhaylcxv
    compile options: '-c'
    gcc: /tmp/tmpvhaylcxv/source.c
    gcc -pthread -B /opt/miniconda3/envs/testbed/compiler_compat -Wl,--sysroot=/ /tmp/tmpvhaylcxv/tmp/tmpvhaylcxv/source.o -L/opt/miniconda3/envs/testbed/lib -lopenblas -o /tmp/tmpvhaylcxv/a.out
      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

      FOUND:
        libraries = ['openblas', 'openblas']
        library_dirs = ['/opt/miniconda3/envs/testbed/lib']
        language = c
        define_macros = [('HAVE_CBLAS', None)]

    running develop
    running build_scripts
    running egg_info
    running build_src
    build_src
    building library "libsvm-skl" sources
    building extension "sklearn.__check_build._check_build" sources
    building extension "sklearn.preprocessing._csr_polynomial_expansion" sources
    building extension "sklearn.cluster._dbscan_inner" sources
    building extension "sklearn.cluster._hierarchical" sources
    building extension "sklearn.cluster._k_means_elkan" sources
    building extension "sklearn.cluster._k_means" sources
    building extension "sklearn.datasets._svmlight_format" sources
    building extension "sklearn.decomposition._online_lda" sources
    building extension "sklearn.decomposition.cdnmf_fast" sources
    building extension "sklearn.ensemble._gradient_boosting" sources
    building extension "sklearn.feature_extraction._hashing" sources
    building extension "sklearn.manifold._utils" sources
    building extension "sklearn.manifold._barnes_hut_tsne" sources
    building extension "sklearn.metrics.cluster.expected_mutual_info_fast" sources
    building extension "sklearn.metrics.pairwise_fast" sources
    building extension "sklearn.neighbors.ball_tree" sources
    building extension "sklearn.neighbors.kd_tree" sources
    building extension "sklearn.neighbors.dist_metrics" sources
    building extension "sklearn.neighbors.typedefs" sources
    building extension "sklearn.neighbors.quad_tree" sources
    building extension "sklearn.tree._tree" sources
    building extension "sklearn.tree._splitter" sources
    building extension "sklearn.tree._criterion" sources
    building extension "sklearn.tree._utils" sources
    building extension "sklearn.utils.sparsefuncs_fast" sources
    building extension "sklearn.utils._cython_blas" sources
    building extension "sklearn.utils.arrayfuncs" sources
    building extension "sklearn.utils.murmurhash" sources
    building extension "sklearn.utils.lgamma" sources
    building extension "sklearn.utils.graph_shortest_path" sources
    building extension "sklearn.utils.fast_dict" sources
    building extension "sklearn.utils.seq_dataset" sources
    building extension "sklearn.utils.weight_vector" sources
    building extension "sklearn.utils._random" sources
    building extension "sklearn.utils._logistic_sigmoid" sources
    building extension "sklearn.svm.libsvm" sources
    building extension "sklearn.svm.liblinear" sources
    building extension "sklearn.svm.libsvm_sparse" sources
    building extension "sklearn.linear_model.cd_fast" sources
    building extension "sklearn.linear_model.sgd_fast" sources
    building extension "sklearn.linear_model.sag_fast" sources
    building extension "sklearn._isotonic" sources
    building data_files sources
    build_src: building npy-pkg config files
    writing scikit_learn.egg-info/PKG-INFO
    writing dependency_links to scikit_learn.egg-info/dependency_links.txt
    writing requirements to scikit_learn.egg-info/requires.txt
    writing top-level names to scikit_learn.egg-info/top_level.txt
    reading manifest file 'scikit_learn.egg-info/SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    adding license file 'COPYING'
    writing manifest file 'scikit_learn.egg-info/SOURCES.txt'
    running build_ext
    customize UnixCCompiler
    customize UnixCCompiler using build_clib
    customize UnixCCompiler
    customize UnixCCompiler using build_ext_subclass
    customize UnixCCompiler
    customize UnixCCompiler using build_ext_subclass
    Creating /opt/miniconda3/envs/testbed/lib/python3.6/site-packages/scikit-learn.egg-link (link to .)
    Adding scikit-learn 0.21.dev0 to easy-install.pth file

    Installed /testbed
    Partial import of sklearn during the build process.
Successfully installed scikit-learn-0.21.dev0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
+ git checkout 3aefc834dce72e850bff48689bea3c7dff5f3fad sklearn/ensemble/tests/test_iforest.py
Updated 0 paths from e85645a93
+ git apply -v -
Checking patch sklearn/ensemble/tests/test_iforest.py...
Applied patch sklearn/ensemble/tests/test_iforest.py cleanly.
+ pytest -rA sklearn/ensemble/tests/test_iforest.py
============================= test session starts ==============================
platform linux -- Python 3.6.13, pytest-6.2.4, py-1.11.0, pluggy-0.13.1
rootdir: /testbed, configfile: setup.cfg
collected 20 items

sklearn/ensemble/tests/test_iforest.py FFFFFFFFFFF.FFFFFFFF              [100%]

=================================== FAILURES ===================================
_________________________________ test_iforest _________________________________

    @pytest.mark.filterwarnings('ignore:threshold_ attribute')
    def test_iforest():
        """Check Isolation Forest for various parameter settings."""
        X_train = np.array([[0, 1], [1, 2]])
        X_test = np.array([[2, 1], [1, 1]])
    
        grid = ParameterGrid({"n_estimators": [3],
                              "max_samples": [0.5, 1.0, 3],
                              "bootstrap": [True, False]})
    
        with ignore_warnings():
            for params in grid:
                IsolationForest(random_state=rng,
>                               **params).fit(X_train).predict(X_test)

sklearn/ensemble/tests/test_iforest.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=True, contamination='legacy',
                max_features=1.0, max_samples=0.5, n_estimators=3, n_jobs=None,
                random_state=RandomState(MT19937) at 0x71A94BAD1DB0, verbose=0)
X = array([[0, 1],
       [1, 2]]), y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
                max_samples = min(256, X.shape[0])
            else:
                raise ValueError("max_samples (%s) is not supported."
                                 "Valid choices are 'auto', int "
                                 "or float." % self.max_samples)
        elif isinstance(self.max_samples, (int, np.integer)):
            if self.max_samples > X.shape[0]:
                warn("max_samples (%s) is greater than the "
                     "total number of samples (%s). max_samples "
                     "will be set to X.shape[0] for estimation."
                     % (self.max_samples, X.shape[0]))
                max_samples = X.shape[0]
            else:
                max_samples = self.max_samples
        elif isinstance(self.max_samples, float):
            if not 0. < self.max_samples <= 1.:
                raise ValueError("max_samples must be in (0, 1], got %r"
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])
        else:
            raise TypeError("max_samples (%s) is not of type integer "
                            "or float." % type(self.max_samples))
    
        if not isinstance(self.bootstrap, bool):
            raise ValueError("`bootstrap` should be an boolean, got {0}.".format(
                self.bootstrap))
    
>       if not (0. < self.contamination <= 0.5):
E       TypeError: '<' not supported between instances of 'float' and 'str'

sklearn/ensemble/iforest.py:264: TypeError
_____________________________ test_iforest_sparse ______________________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:threshold_ attribute')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_iforest_sparse():
        """Check IForest for various parameter settings on sparse input."""
        rng = check_random_state(0)
        X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],
                                                            boston.target[:50],
                                                            random_state=rng)
        grid = ParameterGrid({"max_samples": [0.5, 1.0],
                              "bootstrap": [True, False]})
    
        for sparse_format in [csc_matrix, csr_matrix]:
            X_train_sparse = sparse_format(X_train)
            X_test_sparse = sparse_format(X_test)
    
            for params in grid:
                # Trained on sparse format
                sparse_classifier = IsolationForest(
>                   n_estimators=10, random_state=1, **params).fit(X_train_sparse)

sklearn/ensemble/tests/test_iforest.py:86: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=True, contamination='legacy',
                max_features=1.0, max_samples=0.5, n_estimators=10, n_jobs=None,
                random_state=1, verbose=0)
X = <37x13 sparse matrix of type '<class 'numpy.float64'>'
	with 420 stored elements in Compressed Sparse Column format>
y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
                max_samples = min(256, X.shape[0])
            else:
                raise ValueError("max_samples (%s) is not supported."
                                 "Valid choices are 'auto', int "
                                 "or float." % self.max_samples)
        elif isinstance(self.max_samples, (int, np.integer)):
            if self.max_samples > X.shape[0]:
                warn("max_samples (%s) is greater than the "
                     "total number of samples (%s). max_samples "
                     "will be set to X.shape[0] for estimation."
                     % (self.max_samples, X.shape[0]))
                max_samples = X.shape[0]
            else:
                max_samples = self.max_samples
        elif isinstance(self.max_samples, float):
            if not 0. < self.max_samples <= 1.:
                raise ValueError("max_samples must be in (0, 1], got %r"
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])
        else:
            raise TypeError("max_samples (%s) is not of type integer "
                            "or float." % type(self.max_samples))
    
        if not isinstance(self.bootstrap, bool):
            raise ValueError("`bootstrap` should be an boolean, got {0}.".format(
                self.bootstrap))
    
>       if not (0. < self.contamination <= 0.5):
E       TypeError: '<' not supported between instances of 'float' and 'str'

sklearn/ensemble/iforest.py:264: TypeError
______________________________ test_iforest_error ______________________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:threshold_ attribute')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_iforest_error():
        """Test that it gives proper exception on deficient input."""
        X = iris.data
    
        # Test max_samples
        assert_raises(ValueError,
>                     IsolationForest(max_samples=-1).fit, X)

sklearn/ensemble/tests/test_iforest.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
sklearn/utils/_unittest_backport.py:204: in assertRaises
    return context.handle('assertRaises', args, kwargs)
sklearn/utils/_unittest_backport.py:113: in handle
    callable_obj(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',
                max_features=1.0, max_samples=-1, n_estimators=100, n_jobs=None,
                random_state=None, verbose=0)
X = array([[5.8, 2.8, 5.1, 2.4],
       [6. , 2.2, 4. , 1. ],
       [5.5, 4.2, 1.4, 0.2],
       [7.3, 2.9, 6.3, 1.8],
  ...],
       [6.3, 2.9, 5.6, 1.8],
       [5.8, 2.7, 4.1, 1. ],
       [7.7, 3.8, 6.7, 2.2],
       [4.6, 3.2, 1.4, 0.2]])
y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
                max_samples = min(256, X.shape[0])
            else:
                raise ValueError("max_samples (%s) is not supported."
                                 "Valid choices are 'auto', int "
                                 "or float." % self.max_samples)
        elif isinstance(self.max_samples, (int, np.integer)):
            if self.max_samples > X.shape[0]:
                warn("max_samples (%s) is greater than the "
                     "total number of samples (%s). max_samples "
                     "will be set to X.shape[0] for estimation."
                     % (self.max_samples, X.shape[0]))
                max_samples = X.shape[0]
            else:
                max_samples = self.max_samples
        elif isinstance(self.max_samples, float):
            if not 0. < self.max_samples <= 1.:
                raise ValueError("max_samples must be in (0, 1], got %r"
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])
        else:
            raise TypeError("max_samples (%s) is not of type integer "
                            "or float." % type(self.max_samples))
    
        if not isinstance(self.bootstrap, bool):
            raise ValueError("`bootstrap` should be an boolean, got {0}.".format(
                self.bootstrap))
    
>       if not (0. < self.contamination <= 0.5):
E       TypeError: '<' not supported between instances of 'float' and 'str'

sklearn/ensemble/iforest.py:264: TypeError
__________________________ test_recalculate_max_depth __________________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_recalculate_max_depth():
        """Check max_depth recalculation when max_samples is reset to n_samples"""
        X = iris.data
>       clf = IsolationForest().fit(X)

sklearn/ensemble/tests/test_iforest.py:148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',
                max_features=1.0, max_samples='auto', n_estimators=100,
                n_jobs=None, random_state=None, verbose=0)
X = array([[5.8, 2.8, 5.1, 2.4],
       [6. , 2.2, 4. , 1. ],
       [5.5, 4.2, 1.4, 0.2],
       [7.3, 2.9, 6.3, 1.8],
  ...],
       [6.3, 2.9, 5.6, 1.8],
       [5.8, 2.7, 4.1, 1. ],
       [7.7, 3.8, 6.7, 2.2],
       [4.6, 3.2, 1.4, 0.2]])
y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
                max_samples = min(256, X.shape[0])
            else:
                raise ValueError("max_samples (%s) is not supported."
                                 "Valid choices are 'auto', int "
                                 "or float." % self.max_samples)
        elif isinstance(self.max_samples, (int, np.integer)):
            if self.max_samples > X.shape[0]:
                warn("max_samples (%s) is greater than the "
                     "total number of samples (%s). max_samples "
                     "will be set to X.shape[0] for estimation."
                     % (self.max_samples, X.shape[0]))
                max_samples = X.shape[0]
            else:
                max_samples = self.max_samples
        elif isinstance(self.max_samples, float):
            if not 0. < self.max_samples <= 1.:
                raise ValueError("max_samples must be in (0, 1], got %r"
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])
        else:
            raise TypeError("max_samples (%s) is not of type integer "
                            "or float." % type(self.max_samples))
    
        if not isinstance(self.bootstrap, bool):
            raise ValueError("`bootstrap` should be an boolean, got {0}.".format(
                self.bootstrap))
    
>       if not (0. < self.contamination <= 0.5):
E       TypeError: '<' not supported between instances of 'float' and 'str'

sklearn/ensemble/iforest.py:264: TypeError
__________________________ test_max_samples_attribute __________________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_max_samples_attribute():
        X = iris.data
>       clf = IsolationForest().fit(X)

sklearn/ensemble/tests/test_iforest.py:157: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',
                max_features=1.0, max_samples='auto', n_estimators=100,
                n_jobs=None, random_state=None, verbose=0)
X = array([[5.8, 2.8, 5.1, 2.4],
       [6. , 2.2, 4. , 1. ],
       [5.5, 4.2, 1.4, 0.2],
       [7.3, 2.9, 6.3, 1.8],
  ...],
       [6.3, 2.9, 5.6, 1.8],
       [5.8, 2.7, 4.1, 1. ],
       [7.7, 3.8, 6.7, 2.2],
       [4.6, 3.2, 1.4, 0.2]])
y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
                max_samples = min(256, X.shape[0])
            else:
                raise ValueError("max_samples (%s) is not supported."
                                 "Valid choices are 'auto', int "
                                 "or float." % self.max_samples)
        elif isinstance(self.max_samples, (int, np.integer)):
            if self.max_samples > X.shape[0]:
                warn("max_samples (%s) is greater than the "
                     "total number of samples (%s). max_samples "
                     "will be set to X.shape[0] for estimation."
                     % (self.max_samples, X.shape[0]))
                max_samples = X.shape[0]
            else:
                max_samples = self.max_samples
        elif isinstance(self.max_samples, float):
            if not 0. < self.max_samples <= 1.:
                raise ValueError("max_samples must be in (0, 1], got %r"
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])
        else:
            raise TypeError("max_samples (%s) is not of type integer "
                            "or float." % type(self.max_samples))
    
        if not isinstance(self.bootstrap, bool):
            raise ValueError("`bootstrap` should be an boolean, got {0}.".format(
                self.bootstrap))
    
>       if not (0. < self.contamination <= 0.5):
E       TypeError: '<' not supported between instances of 'float' and 'str'

sklearn/ensemble/iforest.py:264: TypeError
_______________________ test_iforest_parallel_regression _______________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:threshold_ attribute')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_iforest_parallel_regression():
        """Check parallel regression."""
        rng = check_random_state(0)
    
        X_train, X_test, y_train, y_test = train_test_split(boston.data,
                                                            boston.target,
                                                            random_state=rng)
    
        ensemble = IsolationForest(n_jobs=3,
>                                  random_state=0).fit(X_train)

sklearn/ensemble/tests/test_iforest.py:182: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',
                max_features=1.0, max_samples='auto', n_estimators=100,
                n_jobs=3, random_state=0, verbose=0)
X = array([[1.44760e-01, 0.00000e+00, 1.00100e+01, ..., 1.78000e+01,
        3.91500e+02, 1.36100e+01],
       [3.46600e-0...+02, 6.53000e+00],
       [1.40507e+01, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,
        3.50500e+01, 2.12200e+01]])
y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
                max_samples = min(256, X.shape[0])
            else:
                raise ValueError("max_samples (%s) is not supported."
                                 "Valid choices are 'auto', int "
                                 "or float." % self.max_samples)
        elif isinstance(self.max_samples, (int, np.integer)):
            if self.max_samples > X.shape[0]:
                warn("max_samples (%s) is greater than the "
                     "total number of samples (%s). max_samples "
                     "will be set to X.shape[0] for estimation."
                     % (self.max_samples, X.shape[0]))
                max_samples = X.shape[0]
            else:
                max_samples = self.max_samples
        elif isinstance(self.max_samples, float):
            if not 0. < self.max_samples <= 1.:
                raise ValueError("max_samples must be in (0, 1], got %r"
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])
        else:
            raise TypeError("max_samples (%s) is not of type integer "
                            "or float." % type(self.max_samples))
    
        if not isinstance(self.bootstrap, bool):
            raise ValueError("`bootstrap` should be an boolean, got {0}.".format(
                self.bootstrap))
    
>       if not (0. < self.contamination <= 0.5):
E       TypeError: '<' not supported between instances of 'float' and 'str'

sklearn/ensemble/iforest.py:264: TypeError
___________________________ test_iforest_performance ___________________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_iforest_performance():
        """Test Isolation Forest performs well"""
    
        # Generate train/test data
        rng = check_random_state(2)
        X = 0.3 * rng.randn(120, 2)
        X_train = np.r_[X + 2, X - 2]
        X_train = X[:100]
    
        # Generate some abnormal novel observations
        X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))
        X_test = np.r_[X[100:], X_outliers]
        y_test = np.array([0] * 20 + [1] * 20)
    
        # fit the model
>       clf = IsolationForest(max_samples=100, random_state=rng).fit(X_train)

sklearn/ensemble/tests/test_iforest.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',
                max_features=1.0, max_sample...ators=100,
                n_jobs=None,
                random_state=RandomState(MT19937) at 0x71A94B6E2360, verbose=0)
X = array([[-1.25027354e-01, -1.68800482e-02],
       [-6.40858829e-01,  4.92081243e-01],
       [-5.38030676e-01, -2.5252....66428750e-01,  5.41342992e-01],
       [ 5.41229421e-02,  1.65949282e-01],
       [ 3.09908720e-01, -9.87007304e-02]])
y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
                max_samples = min(256, X.shape[0])
            else:
                raise ValueError("max_samples (%s) is not supported."
                                 "Valid choices are 'auto', int "
                                 "or float." % self.max_samples)
        elif isinstance(self.max_samples, (int, np.integer)):
            if self.max_samples > X.shape[0]:
                warn("max_samples (%s) is greater than the "
                     "total number of samples (%s). max_samples "
                     "will be set to X.shape[0] for estimation."
                     % (self.max_samples, X.shape[0]))
                max_samples = X.shape[0]
            else:
                max_samples = self.max_samples
        elif isinstance(self.max_samples, float):
            if not 0. < self.max_samples <= 1.:
                raise ValueError("max_samples must be in (0, 1], got %r"
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])
        else:
            raise TypeError("max_samples (%s) is not of type integer "
                            "or float." % type(self.max_samples))
    
        if not isinstance(self.bootstrap, bool):
            raise ValueError("`bootstrap` should be an boolean, got {0}.".format(
                self.bootstrap))
    
>       if not (0. < self.contamination <= 0.5):
E       TypeError: '<' not supported between instances of 'float' and 'str'

sklearn/ensemble/iforest.py:264: TypeError
___________________________ test_iforest_works[0.25] ___________________________

contamination = 0.25

    @pytest.mark.parametrize("contamination", [0.25, "auto"])
    @pytest.mark.filterwarnings("ignore:threshold_ attribute")
    def test_iforest_works(contamination):
        # toy sample (the last two samples are outliers)
        X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]
    
        # Test IsolationForest
        clf = IsolationForest(
            behaviour="new", random_state=rng, contamination=contamination
        )
>       clf.fit(X)

sklearn/ensemble/tests/test_iforest.py:233: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='new', bootstrap=False, contamination=0.25,
                max_features=1.0, max_samples='a...ators=100,
                n_jobs=None,
                random_state=RandomState(MT19937) at 0x71A94BAD1DB0, verbose=0)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], ...], y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
>               max_samples = min(256, X.shape[0])
E               AttributeError: 'list' object has no attribute 'shape'

sklearn/ensemble/iforest.py:237: AttributeError
___________________________ test_iforest_works[auto] ___________________________

contamination = 'auto'

    @pytest.mark.parametrize("contamination", [0.25, "auto"])
    @pytest.mark.filterwarnings("ignore:threshold_ attribute")
    def test_iforest_works(contamination):
        # toy sample (the last two samples are outliers)
        X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]
    
        # Test IsolationForest
        clf = IsolationForest(
            behaviour="new", random_state=rng, contamination=contamination
        )
>       clf.fit(X)

sklearn/ensemble/tests/test_iforest.py:233: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='new', bootstrap=False, contamination='auto',
                max_features=1.0, max_samples=...ators=100,
                n_jobs=None,
                random_state=RandomState(MT19937) at 0x71A94BAD1DB0, verbose=0)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], ...], y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
>               max_samples = min(256, X.shape[0])
E               AttributeError: 'list' object has no attribute 'shape'

sklearn/ensemble/iforest.py:237: AttributeError
_________________________ test_max_samples_consistency _________________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_max_samples_consistency():
        # Make sure validated max_samples in iforest and BaseBagging are identical
        X = iris.data
>       clf = IsolationForest().fit(X)

sklearn/ensemble/tests/test_iforest.py:246: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',
                max_features=1.0, max_samples='auto', n_estimators=100,
                n_jobs=None, random_state=None, verbose=0)
X = array([[5.8, 2.8, 5.1, 2.4],
       [6. , 2.2, 4. , 1. ],
       [5.5, 4.2, 1.4, 0.2],
       [7.3, 2.9, 6.3, 1.8],
  ...],
       [6.3, 2.9, 5.6, 1.8],
       [5.8, 2.7, 4.1, 1. ],
       [7.7, 3.8, 6.7, 2.2],
       [4.6, 3.2, 1.4, 0.2]])
y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
                max_samples = min(256, X.shape[0])
            else:
                raise ValueError("max_samples (%s) is not supported."
                                 "Valid choices are 'auto', int "
                                 "or float." % self.max_samples)
        elif isinstance(self.max_samples, (int, np.integer)):
            if self.max_samples > X.shape[0]:
                warn("max_samples (%s) is greater than the "
                     "total number of samples (%s). max_samples "
                     "will be set to X.shape[0] for estimation."
                     % (self.max_samples, X.shape[0]))
                max_samples = X.shape[0]
            else:
                max_samples = self.max_samples
        elif isinstance(self.max_samples, float):
            if not 0. < self.max_samples <= 1.:
                raise ValueError("max_samples must be in (0, 1], got %r"
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])
        else:
            raise TypeError("max_samples (%s) is not of type integer "
                            "or float." % type(self.max_samples))
    
        if not isinstance(self.bootstrap, bool):
            raise ValueError("`bootstrap` should be an boolean, got {0}.".format(
                self.bootstrap))
    
>       if not (0. < self.contamination <= 0.5):
E       TypeError: '<' not supported between instances of 'float' and 'str'

sklearn/ensemble/iforest.py:264: TypeError
_______________________ test_iforest_subsampled_features _______________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:threshold_ attribute')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_iforest_subsampled_features():
        # It tests non-regression for #5732 which failed at predict.
        rng = check_random_state(0)
        X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],
                                                            boston.target[:50],
                                                            random_state=rng)
        clf = IsolationForest(max_features=0.8)
>       clf.fit(X_train, y_train)

sklearn/ensemble/tests/test_iforest.py:260: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',
                max_features=0.8, max_samples='auto', n_estimators=100,
                n_jobs=None, random_state=None, verbose=0)
X = array([[5.20177e+00, 0.00000e+00, 1.81000e+01, 1.00000e+00, 7.70000e-01,
        6.12700e+00, 8.34000e+01, 2.72270e+00...      6.89700e+00, 5.43000e+01, 6.33610e+00, 6.00000e+00, 3.00000e+02,
        1.66000e+01, 3.91250e+02, 1.13800e+01]])
y = array([22.7, 26.2, 17.8, 10.9, 22.2, 18.2, 24.6, 22. , 13.4,  5. ,  9.5,
       36.4, 21.6, 31.5, 14.9, 33.3, 24. , 16... , 15.1, 14.9,
       28.4, 14.4, 16.4, 19. , 14.5, 24.4, 31.1, 23.7, 19.3, 32.9, 33.1,
        8.8, 19.9, 26.6, 22. ])

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
                max_samples = min(256, X.shape[0])
            else:
                raise ValueError("max_samples (%s) is not supported."
                                 "Valid choices are 'auto', int "
                                 "or float." % self.max_samples)
        elif isinstance(self.max_samples, (int, np.integer)):
            if self.max_samples > X.shape[0]:
                warn("max_samples (%s) is greater than the "
                     "total number of samples (%s). max_samples "
                     "will be set to X.shape[0] for estimation."
                     % (self.max_samples, X.shape[0]))
                max_samples = X.shape[0]
            else:
                max_samples = self.max_samples
        elif isinstance(self.max_samples, float):
            if not 0. < self.max_samples <= 1.:
                raise ValueError("max_samples must be in (0, 1], got %r"
                                 % self.max_samples)
            max_samples = int(self.max_samples * X.shape[0])
        else:
            raise TypeError("max_samples (%s) is not of type integer "
                            "or float." % type(self.max_samples))
    
        if not isinstance(self.bootstrap, bool):
            raise ValueError("`bootstrap` should be an boolean, got {0}.".format(
                self.bootstrap))
    
>       if not (0. < self.contamination <= 0.5):
E       TypeError: '<' not supported between instances of 'float' and 'str'

sklearn/ensemble/iforest.py:264: TypeError
______________________________ test_score_samples ______________________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_score_samples():
        X_train = [[1, 1], [1, 2], [2, 1]]
>       clf1 = IsolationForest(contamination=0.1).fit(X_train)

sklearn/ensemble/tests/test_iforest.py:288: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=False, contamination=0.1,
                max_features=1.0, max_samples='auto', n_estimators=100,
                n_jobs=None, random_state=None, verbose=0)
X = [[1, 1], [1, 2], [2, 1]], y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
>               max_samples = min(256, X.shape[0])
E               AttributeError: 'list' object has no attribute 'shape'

sklearn/ensemble/iforest.py:237: AttributeError
___________________________ test_iforest_warm_start ____________________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_iforest_warm_start():
        """Test iterative addition of iTrees to an iForest """
    
        rng = check_random_state(0)
        X = rng.randn(20, 2)
    
        # fit first 10 trees
        clf = IsolationForest(n_estimators=10, max_samples=20,
>                             random_state=rng, warm_start=True)
E       TypeError: __init__() got an unexpected keyword argument 'warm_start'

sklearn/ensemble/tests/test_iforest.py:308: TypeError
_______________________________ test_deprecation _______________________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_deprecation():
        X = [[0.0], [1.0]]
        clf = IsolationForest()
    
        assert_warns_message(FutureWarning,
                             'default contamination parameter 0.1 will change '
                             'in version 0.22 to "auto"',
>                            clf.fit, X)

sklearn/ensemble/tests/test_iforest.py:329: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
sklearn/utils/testing.py:196: in assert_warns_message
    result = func(*args, **kw)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',
                max_features=1.0, max_samples='auto', n_estimators=100,
                n_jobs=None, random_state=None, verbose=0)
X = [[0.0], [1.0]], y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
>               max_samples = min(256, X.shape[0])
E               AttributeError: 'list' object has no attribute 'shape'

sklearn/ensemble/iforest.py:237: AttributeError
_____________________________ test_behaviour_param _____________________________

    @pytest.mark.filterwarnings('ignore:default contamination')
    @pytest.mark.filterwarnings('ignore:behaviour="old"')
    def test_behaviour_param():
        X_train = [[1, 1], [1, 2], [2, 1]]
>       clf1 = IsolationForest(behaviour='old').fit(X_train)

sklearn/ensemble/tests/test_iforest.py:347: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='old', bootstrap=False, contamination='legacy',
                max_features=1.0, max_samples='auto', n_estimators=100,
                n_jobs=None, random_state=None, verbose=0)
X = [[1, 1], [1, 2], [2, 1]], y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
>               max_samples = min(256, X.shape[0])
E               AttributeError: 'list' object has no attribute 'shape'

sklearn/ensemble/iforest.py:237: AttributeError
______________________ test_iforest_chunks_works1[0.25-3] ______________________

mocked_get_chunk = <MagicMock name='get_chunk_n_rows' id='124971928106936'>
contamination = 0.25, n_predict_calls = 3

    @patch(
        "sklearn.ensemble.iforest.get_chunk_n_rows",
        side_effect=Mock(**{"return_value": 3}),
    )
    @pytest.mark.parametrize(
        "contamination, n_predict_calls", [(0.25, 3), ("auto", 2)]
    )
    @pytest.mark.filterwarnings("ignore:threshold_ attribute")
    def test_iforest_chunks_works1(
        mocked_get_chunk, contamination, n_predict_calls
    ):
>       test_iforest_works(contamination)

sklearn/ensemble/tests/test_iforest.py:366: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
sklearn/ensemble/tests/test_iforest.py:233: in test_iforest_works
    clf.fit(X)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='new', bootstrap=False, contamination=0.25,
                max_features=1.0, max_samples='a...ators=100,
                n_jobs=None,
                random_state=RandomState(MT19937) at 0x71A94BAD1DB0, verbose=0)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], ...], y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
>               max_samples = min(256, X.shape[0])
E               AttributeError: 'list' object has no attribute 'shape'

sklearn/ensemble/iforest.py:237: AttributeError
______________________ test_iforest_chunks_works1[auto-2] ______________________

mocked_get_chunk = <MagicMock name='get_chunk_n_rows' id='124971932613824'>
contamination = 'auto', n_predict_calls = 2

    @patch(
        "sklearn.ensemble.iforest.get_chunk_n_rows",
        side_effect=Mock(**{"return_value": 3}),
    )
    @pytest.mark.parametrize(
        "contamination, n_predict_calls", [(0.25, 3), ("auto", 2)]
    )
    @pytest.mark.filterwarnings("ignore:threshold_ attribute")
    def test_iforest_chunks_works1(
        mocked_get_chunk, contamination, n_predict_calls
    ):
>       test_iforest_works(contamination)

sklearn/ensemble/tests/test_iforest.py:366: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
sklearn/ensemble/tests/test_iforest.py:233: in test_iforest_works
    clf.fit(X)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='new', bootstrap=False, contamination='auto',
                max_features=1.0, max_samples=...ators=100,
                n_jobs=None,
                random_state=RandomState(MT19937) at 0x71A94BAD1DB0, verbose=0)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], ...], y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
>               max_samples = min(256, X.shape[0])
E               AttributeError: 'list' object has no attribute 'shape'

sklearn/ensemble/iforest.py:237: AttributeError
______________________ test_iforest_chunks_works2[0.25-3] ______________________

mocked_get_chunk = <MagicMock name='get_chunk_n_rows' id='124971932614496'>
contamination = 0.25, n_predict_calls = 3

    @patch(
        "sklearn.ensemble.iforest.get_chunk_n_rows",
        side_effect=Mock(**{"return_value": 10}),
    )
    @pytest.mark.parametrize(
        "contamination, n_predict_calls", [(0.25, 3), ("auto", 2)]
    )
    @pytest.mark.filterwarnings("ignore:threshold_ attribute")
    def test_iforest_chunks_works2(
        mocked_get_chunk, contamination, n_predict_calls
    ):
>       test_iforest_works(contamination)

sklearn/ensemble/tests/test_iforest.py:382: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
sklearn/ensemble/tests/test_iforest.py:233: in test_iforest_works
    clf.fit(X)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='new', bootstrap=False, contamination=0.25,
                max_features=1.0, max_samples='a...ators=100,
                n_jobs=None,
                random_state=RandomState(MT19937) at 0x71A94BAD1DB0, verbose=0)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], ...], y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
>               max_samples = min(256, X.shape[0])
E               AttributeError: 'list' object has no attribute 'shape'

sklearn/ensemble/iforest.py:237: AttributeError
______________________ test_iforest_chunks_works2[auto-2] ______________________

mocked_get_chunk = <MagicMock name='get_chunk_n_rows' id='124971928226672'>
contamination = 'auto', n_predict_calls = 2

    @patch(
        "sklearn.ensemble.iforest.get_chunk_n_rows",
        side_effect=Mock(**{"return_value": 10}),
    )
    @pytest.mark.parametrize(
        "contamination, n_predict_calls", [(0.25, 3), ("auto", 2)]
    )
    @pytest.mark.filterwarnings("ignore:threshold_ attribute")
    def test_iforest_chunks_works2(
        mocked_get_chunk, contamination, n_predict_calls
    ):
>       test_iforest_works(contamination)

sklearn/ensemble/tests/test_iforest.py:382: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
sklearn/ensemble/tests/test_iforest.py:233: in test_iforest_works
    clf.fit(X)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = IsolationForest(behaviour='new', bootstrap=False, contamination='auto',
                max_features=1.0, max_samples=...ators=100,
                n_jobs=None,
                random_state=RandomState(MT19937) at 0x71A94BAD1DB0, verbose=0)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], ...], y = None

    def fit(self, X, y=None):
        """Fit estimator.
    
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency.
        y : Ignored
            Not used, present for API consistency by convention.
    
        Returns
        -------
        self : object
    
        """
        # Check parameters
        if not isinstance(self.n_estimators, int):
            raise ValueError("n_estimators must be an integer, "
                             "got {0}.".format(type(self.n_estimators)))
        elif self.n_estimators <= 0:
            raise ValueError("n_estimators must be greater than zero, "
                             "got {0}.".format(self.n_estimators))
    
        if isinstance(self.max_samples, str):
            if self.max_samples == "auto":
>               max_samples = min(256, X.shape[0])
E               AttributeError: 'list' object has no attribute 'shape'

sklearn/ensemble/iforest.py:237: AttributeError
==================================== PASSES ====================================
=========================== short test summary info ============================
PASSED sklearn/ensemble/tests/test_iforest.py::test_iforest_average_path_length
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest - TypeError: '<' ...
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_sparse - TypeErro...
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_error - TypeError...
FAILED sklearn/ensemble/tests/test_iforest.py::test_recalculate_max_depth - T...
FAILED sklearn/ensemble/tests/test_iforest.py::test_max_samples_attribute - T...
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_parallel_regression
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_performance - Typ...
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_works[0.25] - Att...
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_works[auto] - Att...
FAILED sklearn/ensemble/tests/test_iforest.py::test_max_samples_consistency
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_subsampled_features
FAILED sklearn/ensemble/tests/test_iforest.py::test_score_samples - Attribute...
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_warm_start - Type...
FAILED sklearn/ensemble/tests/test_iforest.py::test_deprecation - AttributeEr...
FAILED sklearn/ensemble/tests/test_iforest.py::test_behaviour_param - Attribu...
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_chunks_works1[0.25-3]
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_chunks_works1[auto-2]
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_chunks_works2[0.25-3]
FAILED sklearn/ensemble/tests/test_iforest.py::test_iforest_chunks_works2[auto-2]
========================= 19 failed, 1 passed in 0.98s =========================
+ git checkout 3aefc834dce72e850bff48689bea3c7dff5f3fad sklearn/ensemble/tests/test_iforest.py
Updated 1 path from e85645a93
